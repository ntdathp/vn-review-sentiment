{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b03f9a",
   "metadata": {},
   "source": [
    "\n",
    "# PhoBERT 5-Classes Sentiment Inference (Notebook)\n",
    "\n",
    "Notebook ho√° t·ª´ script inference c·ªßa b·∫°n.  \n",
    "- H·ªó tr·ª£ c·∫£ **.txt** (m·ªói d√≤ng 1 c√¢u) v√† **.csv** (c·ªôt `text`, tu·ª≥ ch·ªçn `label`)  \n",
    "- Ti·ªÅn x·ª≠ l√Ω gi·ªëng l√∫c train (normalize + optional word segmentation)  \n",
    "- Tu·ª≥ ch·ªçn **neutral penalty** (ƒëi·ªÅu ch·ªânh logit l·ªõp *neutral*)  \n",
    "- Xu·∫•t **CSV** d·ª± ƒëo√°n (+ x√°c su·∫•t) n·∫øu mu·ªën\n",
    "\n",
    "> **G·ª£i √Ω ch·∫°y:** S·ª≠a `CFG` ·ªü √¥ **Config** (ƒë∆∞·ªùng d·∫´n model/input/output), sau ƒë√≥ ch·∫°y l·∫ßn l∆∞·ª£t c√°c √¥ t·ª´ tr√™n xu·ªëng.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a0954",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2c006b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(model_dir='/home/dat/llm_ws/phobert/phobert_5cls_clean', input_txt='', input_csv='/home/dat/llm_ws/data/test/vn_product_reviews_test_100_challenge.csv', max_len=160, batch_size=64, show=20, normalize=True, use_seg=False, neutral_penalty=0.0, out_csv='')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# === S·ª≠a l·∫°i c·∫•u h√¨nh t·∫°i ƒë√¢y ===\n",
    "CFG = SimpleNamespace(\n",
    "    model_dir=\"/home/dat/llm_ws/phobert/phobert_5cls_clean\",       # Th∆∞ m·ª•c model ƒë√£ save_model()\n",
    "    input_txt=\"\",                                           # File .txt (m·ªói d√≤ng 1 c√¢u). ƒê·ªÉ \"\" n·∫øu d√πng CSV\n",
    "    input_csv=\"/home/dat/llm_ws/data/test/vn_product_reviews_test_100_challenge.csv\",  # File .csv (c·ªôt 'text', optional 'label')\n",
    "    max_len=160,\n",
    "    batch_size=64,\n",
    "    show=20,                                                # S·ªë d√≤ng print ra\n",
    "    normalize=True,\n",
    "    use_seg=False,                                          # B·∫≠t word segmentation n·∫øu l√∫c train c√≥ b·∫≠t\n",
    "    neutral_penalty=0.0,                                    # V√≠ d·ª•: -0.2 ƒë·ªÉ \"ph·∫°t\" neutral\n",
    "    out_csv=\"\"                                              # ƒê∆∞·ªùng d·∫´n CSV ƒë·ªÉ l∆∞u (v√≠ d·ª•: \"/home/dat/llm_ws/out/preds.csv\"); \"\" n·∫øu kh√¥ng l∆∞u\n",
    ")\n",
    "print(CFG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d435f19",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "207d1844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "LABELS_5 = [\"very_negative\",\"negative\",\"neutral\",\"positive\",\"very_positive\"]\n",
    "LBL2ID = {l:i for i,l in enumerate(LABELS_5)}\n",
    "ID2LBL = {i:l for l,i in LBL2ID.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc13332",
   "metadata": {},
   "source": [
    "## Ti·ªÅn x·ª≠ l√Ω & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d14df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== same normalize as training ====\n",
    "EMO_POS = [\"ü§©\",\"ü•∞\",\"üòç\",\"‚ù§Ô∏è\",\"üëç\",\"üòé\",\"üëå\",\"‚ú®\",\"üî•\",\"üíØ\"]\n",
    "EMO_NEG = [\"üò±\",\"üò°\",\"ü§¨\",\"üí©\",\"üëé\",\"üò§\",\"üòû\",\"üò≠\"]\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    for e in EMO_POS: s = s.replace(e, \" EMO_POS \")\n",
    "    for e in EMO_NEG: s = s.replace(e, \" EMO_NEG \")\n",
    "    repl = {\n",
    "        \"vl\": \"r·∫•t\", \"okeee\": \"ok\", \"∆∞ng\": \"r·∫•t th√≠ch\",\n",
    "        \"si√™u si√™u\": \"r·∫•t\", \"si√™u th·∫•t v·ªçng\": \"r·∫•t th·∫•t v·ªçng\",\n",
    "        \"m√£i ƒë·ªânh\": \"r·∫•t t·ªët\", \"best of best\": \"r·∫•t t·ªët\", \"best choice\": \"r·∫•t t·ªët\",\n",
    "        \"ƒë·ªânh c·ªßa ch√≥p\": \"r·∫•t t·ªët\",\n",
    "    }\n",
    "    for k,v in repl.items():\n",
    "        s = re.sub(rf\"\\b{re.escape(k)}\\b\", v, s, flags=re.IGNORECASE)\n",
    "    return s\n",
    "\n",
    "def maybe_segment(text, use_seg=False):\n",
    "    if not use_seg: return text\n",
    "    try:\n",
    "        from underthesea import word_tokenize\n",
    "        return word_tokenize(text, format=\"text\")\n",
    "    except Exception as e:\n",
    "        print(\"[C·∫£nh b√°o] Kh√¥ng th·ªÉ import underthesea. T·∫Øt use_seg ho·∫∑c c√†i ƒë·∫∑t th∆∞ vi·ªán. L·ªói:\", e)\n",
    "        return text\n",
    "\n",
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e, axis=-1, keepdims=True)\n",
    "\n",
    "def load_texts_from_txt(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [l.strip() for l in f if l.strip()]\n",
    "    return lines, None  # no labels\n",
    "\n",
    "def load_texts_from_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    assert \"text\" in df.columns, \"CSV ph·∫£i c√≥ c·ªôt 'text'\"\n",
    "    texts = df[\"text\"].astype(str).tolist()\n",
    "    labels = None\n",
    "    if \"label\" in df.columns:\n",
    "        labels = [LBL2ID[l] if l in LBL2ID else None for l in df[\"label\"].astype(str)]\n",
    "    return texts, labels, df\n",
    "\n",
    "def batched(iterable, n):\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i+n]\n",
    "\n",
    "def apply_preproc(texts, normalize=True, use_seg=False):\n",
    "    out = []\n",
    "    for t in texts:\n",
    "        s = normalize_text(t) if normalize else t\n",
    "        s = maybe_segment(s, use_seg=use_seg)\n",
    "        out.append(s)\n",
    "    return out\n",
    "\n",
    "def maybe_apply_class_bias(logits, bias_vec):\n",
    "    # bias_vec: list of floats length=5, added to logits (logit adjustment)\n",
    "    if bias_vec is None: return logits\n",
    "    b = np.array(bias_vec, dtype=np.float32).reshape(1, -1)\n",
    "    return logits + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15040c7",
   "metadata": {},
   "source": [
    "## Load model/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09deb801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loaded model from: /home/dat/llm_ws/phobert/phobert_5cls_clean\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "tok = AutoTokenizer.from_pretrained(CFG.model_dir, use_fast=False)\n",
    "mdl = AutoModelForSequenceClassification.from_pretrained(CFG.model_dir).to(device).eval()\n",
    "print(\"Loaded model from:\", CFG.model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a5914",
   "metadata": {},
   "source": [
    "## N·∫°p d·ªØ li·ªáu ƒë·∫ßu v√†o (.txt ho·∫∑c .csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bebf5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë d√≤ng input: 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texts, labels, df_src = None, None, None\n",
    "if CFG.input_txt:\n",
    "    texts, _ = load_texts_from_txt(CFG.input_txt)\n",
    "elif CFG.input_csv:\n",
    "    texts, labels, df_src = load_texts_from_csv(CFG.input_csv)\n",
    "else:\n",
    "    raise SystemExit(\"C·∫ßn c·∫•u h√¨nh input_txt ho·∫∑c input_csv\")\n",
    "\n",
    "if len(texts) == 0:\n",
    "    raise SystemExit(\"Kh√¥ng c√≥ c√¢u n√†o ƒë·ªÉ d·ª± ƒëo√°n.\")\n",
    "\n",
    "print(f\"S·ªë d√≤ng input: {len(texts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a3188f",
   "metadata": {},
   "source": [
    "## Ti·ªÅn x·ª≠ l√Ω (gi·ªëng train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38fd1dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V√≠ d·ª• sau ti·ªÅn x·ª≠ l√Ω: loa h·ªèng ngay l·∫ßn 1, giao h√†ng k·∫πt m√£i.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texts_proc = apply_preproc(texts, normalize=CFG.normalize, use_seg=CFG.use_seg)\n",
    "print(\"V√≠ d·ª• sau ti·ªÅn x·ª≠ l√Ω:\", texts_proc[0] if texts_proc else \"(empty)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874bc53",
   "metadata": {},
   "source": [
    "## D·ª± ƒëo√°n theo batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56f49777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ho√†n t·∫•t d·ª± ƒëo√°n.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for chunk in batched(texts_proc, CFG.batch_size):\n",
    "        enc = tok(chunk, truncation=True, padding=True, max_length=CFG.max_len, return_tensors=\"pt\")\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        logits = mdl(**enc).logits.detach().cpu().numpy()\n",
    "        all_logits.append(logits)\n",
    "logits = np.concatenate(all_logits, axis=0)\n",
    "\n",
    "# optional class-bias (e.g., penalize neutral)\n",
    "bias = None\n",
    "if CFG.neutral_penalty != 0.0:\n",
    "    bias = [0.0, 0.0, CFG.neutral_penalty, 0.0, 0.0]  # add to logits\n",
    "logits = maybe_apply_class_bias(logits, bias)\n",
    "\n",
    "probs = softmax(logits)\n",
    "pred_ids = probs.argmax(-1)\n",
    "pred_labels = [ID2LBL[int(i)] for i in pred_ids]\n",
    "pmax = probs.max(axis=1)\n",
    "\n",
    "print(\"Ho√†n t·∫•t d·ª± ƒëo√°n.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3f71d2",
   "metadata": {},
   "source": [
    "## Hi·ªÉn th·ªã nhanh (first N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bdc6f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Predictions (first N) ===\n",
      "very_negative   0.951  | loa h·ªèng ngay l·∫ßn 1, giao h√†ng k·∫πt m√£i.\n",
      "neutral         0.853  | robot hut bui kh√¥ng n·ªïi b·∫≠t, gi√° t·∫°m ·ªïn.\n",
      "positive        0.958  | S·∫£n ph·∫©m robot hut bui x·ªãn s√≤ vl, d√πng kh√° ok.\n",
      "positive        0.958  | S·∫£n ph·∫©m laptop ƒë·ªânh cao ü§©, d√πng kh√° ok.\n",
      "very_positive   0.955  | tai nghe kh√¥ng ch√™ v√†o ƒë√¢u ƒëc, qu√° y√™u.\n",
      "negative        0.956  | M√¨nh th·∫•y chuot kh√≥ ch·ªãu vc, ch·∫•t l∆∞·ª£ng ch∆∞a ·ªïn.\n",
      "neutral         0.877  | chuot ·ªïn ƒë·ªÉ d√πng vƒÉn ph√≤ng, gi√° t·∫°m ·ªïn.\n",
      "neutral         0.881  | loa t·∫°m dc, gi√° t·∫°m ·ªïn.\n",
      "very_negative   0.952  | laptop th·∫£m h·ªça üò±, giao h√†ng k·∫πt m√£i.\n",
      "positive        0.957  | S·∫£n ph·∫©m man hinh m∆∞·ª£t m√† 10/10, d√πng kh√° ok.\n",
      "negative        0.957  | M√¨nh th·∫•y man hinh kh√≥ ch·ªãu vc, ch·∫•t l∆∞·ª£ng ch∆∞a ·ªïn.\n",
      "neutral         0.871  | chuot kh√¥ng n·ªïi b·∫≠t, gi√° t·∫°m ·ªïn.\n",
      "negative        0.956  | M√¨nh th·∫•y chuot rep ch·∫≠m, ch·∫•t l∆∞·ª£ng ch∆∞a ·ªïn.\n",
      "positive        0.958  | S·∫£n ph·∫©m robot hut bui ƒë·ªânh cao ü§©, d√πng kh√° ok.\n",
      "positive        0.959  | S·∫£n ph·∫©m dong ho ok ph·∫øt üòé, d√πng kh√° ok.\n",
      "neutral         0.884  | loa b√¨nh th∆∞·ªùng nh∆∞ bao sp kh√°c, gi√° t·∫°m ·ªïn.\n",
      "positive        0.956  | S·∫£n ph·∫©m laptop best choice, d√πng kh√° ok.\n",
      "very_positive   0.957  | dong ho 10/10 perfect, qu√° y√™u.\n",
      "positive        0.958  | S·∫£n ph·∫©m tai nghe qu√° okeee, d√πng kh√° ok.\n",
      "neutral         0.883  | loa ƒë√∫ng nh∆∞ m√¥ t·∫£, gi√° h·ª£p l√Ω.\n",
      "... (80 more)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "show_n = min(len(texts), CFG.show)\n",
    "print(\"\\n=== Predictions (first N) ===\")\n",
    "for t, pi, p in zip(texts[:show_n], pred_ids[:show_n], pmax[:show_n]):\n",
    "    print(f\"{ID2LBL[int(pi)]:14s}  {p:0.3f}  | {t}\")\n",
    "if len(texts) > show_n:\n",
    "    print(f\"... ({len(texts)-show_n} more)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001410fb",
   "metadata": {},
   "source": [
    "## Metrics (n·∫øu c√≥ nh√£n h·ª£p l·ªá trong CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9313b0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Metrics (on rows with valid labels) ===\n",
      "Accuracy : 1.0000\n",
      "Macro F1 : 1.0000\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[20  0  0  0  0]\n",
      " [ 0 20  0  0  0]\n",
      " [ 0  0 20  0  0]\n",
      " [ 0  0  0 20  0]\n",
      " [ 0  0  0  0 20]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "very_negative     1.0000    1.0000    1.0000        20\n",
      "     negative     1.0000    1.0000    1.0000        20\n",
      "      neutral     1.0000    1.0000    1.0000        20\n",
      "     positive     1.0000    1.0000    1.0000        20\n",
      "very_positive     1.0000    1.0000    1.0000        20\n",
      "\n",
      "     accuracy                         1.0000       100\n",
      "    macro avg     1.0000    1.0000    1.0000       100\n",
      " weighted avg     1.0000    1.0000    1.0000       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if labels is not None and any(l is not None for l in labels):\n",
    "    idx = [i for i,l in enumerate(labels) if l is not None]\n",
    "    y_true = np.array([labels[i] for i in idx], dtype=int)\n",
    "    y_pred = pred_ids[idx]\n",
    "    print(\"\\n=== Metrics (on rows with valid labels) ===\")\n",
    "    print(f\"Accuracy : {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Macro F1 : {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "    print(\"\\nConfusion matrix (rows=true, cols=pred):\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=LABELS_5, digits=4))\n",
    "else:\n",
    "    print(\"Kh√¥ng c√≥ c·ªôt label ho·∫∑c kh√¥ng c√≥ nh√£n h·ª£p l·ªá => b·ªè qua t√≠nh metrics.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6013d",
   "metadata": {},
   "source": [
    "## L∆∞u CSV (tu·ª≥ ch·ªçn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c89c5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG.out_csv r·ªóng => kh√¥ng l∆∞u CSV.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if CFG.out_csv:\n",
    "    if df_src is None:\n",
    "        df_out = pd.DataFrame({\"text\": texts})\n",
    "    else:\n",
    "        df_out = df_src.copy()\n",
    "    df_out[\"_pred\"] = pred_labels\n",
    "    df_out[\"_pmax\"] = pmax\n",
    "    for i, name in enumerate(LABELS_5):\n",
    "        df_out[f\"prob_{name}\"] = probs[:, i]\n",
    "    os.makedirs(os.path.dirname(CFG.out_csv) or \".\", exist_ok=True)\n",
    "    df_out.to_csv(CFG.out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[Saved] {CFG.out_csv}\")\n",
    "else:\n",
    "    print(\"CFG.out_csv r·ªóng => kh√¥ng l∆∞u CSV.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda2c03",
   "metadata": {},
   "source": [
    "## Demo nhanh (m·ªôt c√¢u on-the-fly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02234d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: san pham qua toi te\n",
      "Pred: positive\n",
      "Probs: {'very_negative': 0.013087497092783451, 'negative': 0.011440116912126541, 'neutral': 0.00748001504689455, 'positive': 0.9563746452331543, 'very_positive': 0.011617729440331459}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "demo_text = \"san pham qua toi te\"   \n",
    "demo_proc = apply_preproc([demo_text], normalize=CFG.normalize, use_seg=CFG.use_seg)\n",
    "\n",
    "with torch.no_grad():\n",
    "    enc = tok(demo_proc, truncation=True, padding=True, max_length=CFG.max_len, return_tensors=\"pt\")\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    logit = mdl(**enc).logits.detach().cpu().numpy()[0]\n",
    "\n",
    "if CFG.neutral_penalty != 0.0:\n",
    "    logit = maybe_apply_class_bias(logit[None, :], [0.0, 0.0, CFG.neutral_penalty, 0.0, 0.0])[0]\n",
    "\n",
    "prob = softmax(logit[None, :])[0]\n",
    "pred = ID2LBL[int(prob.argmax())]\n",
    "print(\"Text:\", demo_text)\n",
    "print(\"Pred:\", pred)\n",
    "print(\"Probs:\", {LABELS_5[i]: float(prob[i]) for i in range(len(LABELS_5))})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
