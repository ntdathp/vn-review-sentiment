#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, json, random, argparse, unicodedata, re, math
from dataclasses import dataclass
from typing import Dict, List, Optional
import numpy as np
import torch
from datasets import load_dataset, Dataset, DatasetDict
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer
)
from transformers.generation.logits_process import LogitsProcessor, LogitsProcessorList

# ===================== Teencode / Emoji (cho b·∫©n ho√° & decode constraint) =====================
EMO_POS = ["ü§©","ü•∞","üòç","‚ù§Ô∏è","üëç","üòé","üëå","‚ú®","üî•","üíØ","‚ù§","‚ô•","üíï","üíñ","üíó","üíì","üíû","üíò","üíù","üíü","üòÑ","üòÅ","üòÉ","üôÇ","üòä","üòå","ü§ó","üëè","üôå","ü´∂","‚≠ê","üåü","‚ö°","üéâ","ü•≥","üîù","üÜí","üÜó","‚úÖ"]
EMO_NEG = ["üò±","üò°","ü§¨","üí©","üëé","üò§","üòû","üò≠","üò†","üòñ","üò£","üò´","üò©","ü•µ","ü•∂","ü§¢","ü§Æ","üò∑","‚ö†","‚ùå","üÜò","üíî","ü•≤","üòì","üòî","üòï"]

# slang ti√™u c·ª±c ph·ªï bi·∫øn (ƒë·ªÉ dirtyfy & option c·∫•m khi decode)
SLANG_NEG = {"vcl", "vkl", "vl"}

TEENCODE_INV: Dict[str, List[str]] = {
    "kh√¥ng": ["ko","kh","k","khong","hong","h√¥ng","hok","hem","h√™m","h√¥ng c√≥","hong co"],
    "ch∆∞a": ["chua","chz","ch∆∞aaa","ch v·∫´n"],
    "r·∫•t": ["rat","r·∫•ttt","vl","vcl","vvcl","c·ª±c","c·ª±c k·ª≥","c·ª±c k√¨","ck","max","si√™u"],
    "qu√°": ["qua","w√°","qa","q√°","quaaaa","vl","vcl"],
    "h∆°i": ["hoi","h","h∆°i b·ªã"],
    "·ªïn": ["on","·ªïn √°p","ok","oke","okela","okla"],

    "c√≥": ["co","c","c√≥a","c√≥ √°","y","yes","yep","yup"],
    "ƒë∆∞·ª£c": ["dc","ƒëc","ƒëk","ok","oke","okie","oki","oklah"],
    "r·ªìi": ["r","roi","r√πi","r n√®","r nha"],
    "ƒë√∫ng": ["dung","chu·∫©n","ch√≠nh x√°c","chuan","chu·∫©n b√†i"],

    "t√¥i": ["t","toi","tui","t·ªõ","m·ªÅnh"],
    "m√¨nh": ["mk","mik","m","minh","m√≠nh"],
    "b·∫°n": ["b","bn","b ∆°i","bro","c·∫≠u"],

    "g√¨": ["j","ji","cj","c√°i j"],
    "t·∫°i sao": ["ts","v√¨ sao","sao dz","sao z"],
    "nh∆∞ th·∫ø n√†o": ["ntn","nh∆∞ n√†o"],
    "v·∫≠y": ["v","z","dz"],
    "b√¢y gi·ªù": ["bg","bh"],

    "s·∫£n ph·∫©m": ["sp","s/ph·∫©m","spham","san pham","s·∫£n ph"],
    "ƒë∆°n h√†ng": ["ƒëh","don hang","ƒë∆°n"],
    "khuy·∫øn m√£i": ["km","sale","gg","flash sale","fs"],
    "gi·∫£m gi√°": ["gg","sale off","down gi√°"],
    "qu·∫£ng c√°o": ["qc","ads"],
    "b·∫£o h√†nh": ["bh","bao hanh"],
    "ch√≠nh h√£ng": ["chh","auth","authen"],

    "giao h√†ng": ["ship","gh","giao","giao l·∫π","ship l·∫π"],
    "ƒë√≥ng g√≥i": ["ƒëg","dong goi","pack","package","ƒë√≥ng g"],

    "ƒë√°ng ti·ªÅn": ["ƒë√°ng l·∫Øm","x·ª©ng ƒë√°ng"],
    "th·∫•t v·ªçng": ["that vong","tv","t·ª•t mood","si√™u th·∫•t v·ªçng"],
    "t·ªá": ["te","t·ªá vl","t·ªá vcl"],
    "k√©m": ["kem","d·ªüm"],

    "giao nhanh": ["ship nhanh","giao c·∫•p t·ªëc"],
    "ƒë√≥ng g√≥i c·∫©n th·∫≠n": ["ƒëg kƒ©","pack kƒ©","ƒë√≥ng g kƒ©","ƒë√≥ng g√≥i k·ªπ","pack k·ªπ"],
    "ƒë√∫ng m√¥ t·∫£": ["ƒë√∫ng nh∆∞ m√¥ t·∫£","ƒë√∫ng nh∆∞ h√¨nh"],
    "kh√¥ng nh∆∞ m√¥ t·∫£": ["kh√¥ng ƒë√∫ng m√¥ t·∫£","kh√¥ng gi·ªëng h√¨nh","ko nh∆∞ mta"],

    # v√†i c·ª•m review ‚Äúƒë√≠ch‚Äù ƒë·ªÉ dirtyfy ra ‚Äúg ·∫©u‚Äù, ‚Äúg ·∫£o‚Äù, ‚Ä¶
    "ƒë√≥ng g√≥i s∆° s√†i": ["ƒë√≥ng ·∫©u","g√≥i ·∫©u","ƒë√≥ng g ·∫©u","g ·∫©u"],
}

# ===================== Utils cho dirtyfy =====================
def nfc(s): return unicodedata.normalize("NFC", s)

def strip_accents_simple(s: str) -> str:
    s = unicodedata.normalize("NFD", s).replace("ƒë","d").replace("ƒê","D")
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")
    return unicodedata.normalize("NFC", s)

def maybe_drop_diacritics(sent: str, p=0.5):
    return strip_accents_simple(sent) if random.random() < p else sent

def apply_teencode_phrases(sent: str, prob=0.35):
    s = sent
    if random.random() > prob: return s
    keys = sorted([k for k in TEENCODE_INV if " " in k], key=lambda k: -len(k))
    for k in keys:
        if k in s and random.random() < 0.6:
            s = s.replace(k, random.choice(TEENCODE_INV[k]))
    return s

def apply_teencode_words(sent: str, prob=0.6):
    if random.random() > prob: return sent
    toks = sent.split()
    for i,w in enumerate(toks):
        lw = w.lower()
        if lw in TEENCODE_INV and " " not in lw:
            repl = random.choice(TEENCODE_INV[lw])
            if w.isupper(): repl = repl.upper()
            elif w[0].isupper(): repl = repl[0].upper() + repl[1:]
            toks[i] = repl
    return " ".join(toks)

def maybe_insert_emojis(sent: str, prob=0.5):
    if random.random() > prob: return sent
    if re.search(r"(tuy·ªát v·ªùi|ƒë·ªânh|r·∫•t th√≠ch|h√†i l√≤ng)", sent, flags=re.I): return sent + " " + random.choice(EMO_POS)
    if re.search(r"(t·ªá|k√©m|th·∫•t v·ªçng|kh√¥ng h√†i l√≤ng|x·∫•u|s∆° s√†i)", sent, flags=re.I): return sent + " " + random.choice(EMO_NEG)
    return sent

def dirtyfy(clean: str) -> str:
    s = clean
    s = maybe_drop_diacritics(s, 0.55)
    s = apply_teencode_phrases(s, 0.35)
    s = apply_teencode_words(s, 0.6)
    # th·ªânh tho·∫£ng tr·ªôn slang ti√™u c·ª±c
    if random.random() < 0.3:
        s += " " + random.choice(list(SLANG_NEG))
    s = maybe_insert_emojis(s, 0.5)
    if random.random() < 0.6: s = s.lower()
    return nfc(re.sub(r"\s+", " ", s).strip())

# ===================== Synthetic t·ª´ template canonical =====================
BUILTIN_TEMPLATES = [
    # ch·∫•t l∆∞·ª£ng / logistic / ti√™u c·ª±c
    "S·∫£n ph·∫©m ƒë√≥ng g√≥i s∆° s√†i, t√¥i r·∫•t th·∫•t v·ªçng.",
    "S·∫£n ph·∫©m kh√¥ng nh∆∞ m√¥ t·∫£.",
    "ƒê√≥ng g√≥i c·∫©n th·∫≠n, giao nhanh.",
    "S·∫£n ph·∫©m k√©m ch·∫•t l∆∞·ª£ng.",
    "T√¥i kh√¥ng h√†i l√≤ng v·ªÅ ƒë√≥ng g√≥i.",
    "S·∫£n ph·∫©m ƒë√°ng ti·ªÅn.",
    "S·∫£n ph·∫©m qu√° t·ªá.",
    "S·∫£n ph·∫©m ch√≠nh h√£ng.",
    "D·ªãch v·ª• giao h√†ng ch·∫≠m.",
    "T√¥i mu·ªën ƒë·ªïi tr·∫£ s·∫£n ph·∫©m.",
]

def build_synth_from_templates(templates: List[str], variants_per: int = 6) -> DatasetDict:
    src, tgt = [], []
    for t in templates:
        tgt.append(t)
        src.append(t)  # b·∫£n s·∫°ch g·ªëc
        for _ in range(variants_per):
            src.append(dirtyfy(t))
            tgt.append(t)
    data = {"src": src, "tgt": tgt}
    ds = Dataset.from_dict(data)
    ds = ds.train_test_split(test_size=0.1, seed=42)
    return DatasetDict(train=ds["train"], validation=ds["test"])

# ===================== Metric ƒë∆°n gi·∫£n =====================
def chrF1(ref: str, hyp: str) -> float:
    ref_set, hyp_set = set(ref), set(hyp)
    inter = len(ref_set & hyp_set)
    if inter == 0: return 0.0
    p = inter / max(1, len(hyp_set)); r = inter / max(1, len(ref_set))
    return 0.0 if p + r == 0 else 2 * p * r / (p + r)

@dataclass
class DataArgs:
    model_name: str = "google/mt5-small"
    max_src_len: int = 96
    max_tgt_len: int = 96

# ===================== LogitsProcessor ch·∫∑n chu·ªói con =====================
class ForbidSubstringsProcessor(LogitsProcessor):
    def __init__(self, tokenizer, banned_substrings: List[str]):
        self.tok = tokenizer
        # chu·∫©n ho√° NFC ƒë·ªÉ so tr√πng ch·∫Øc ch·∫Øn
        self.banned = [unicodedata.normalize("NFC", s) for s in banned_substrings]

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        # input_ids: (num_beams, cur_len), scores: (num_beams, vocab_size)
        vocab_size = scores.shape[-1]
        # l·∫∑p qua t·ª´ng beam
        for i in range(input_ids.size(0)):
            prefix = unicodedata.normalize("NFC", self.tok.decode(input_ids[i], skip_special_tokens=True))
            # th·ª≠ c·ªông t·ª´ng token ·ª©ng vi√™n, n·∫øu t·∫°o ra chu·ªói b·ªã c·∫•m ‚Üí -inf
            # l∆∞u √Ω: c√≥ th·ªÉ t·ªëi ∆∞u top-k; ƒë·ªÉ ƒë∆°n gi·∫£n c·ª© qu√©t h·∫øt vocab
            for tid in range(vocab_size):
                if torch.isneginf(scores[i, tid]):
                    continue
                piece = self.tok.decode([tid], skip_special_tokens=True)
                if not piece:
                    continue
                candidate = unicodedata.normalize("NFC", prefix + piece)
                if any(bad in candidate for bad in self.banned):
                    scores[i, tid] = float("-inf")
        return scores

# ===================== Main =====================
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--mode", choices=["train","infer"], default="train")
    ap.add_argument("--model_name", default="google/mt5-small")
    ap.add_argument("--output_dir", required=True)

    # d·ªØ li·ªáu
    ap.add_argument("--train_csv", default=None)
    ap.add_argument("--dev_csv",   default=None)
    ap.add_argument("--split_ratio", type=float, default=0.1)
    ap.add_argument("--train_jsonl", default=None)
    ap.add_argument("--clean_txt", default=None)
    ap.add_argument("--num_variants", type=int, default=3)

    # b·∫≠t synthetic t·ª´ template canonical
    ap.add_argument("--use_builtin_templates", action="store_true",
                    help="Tr·ªôn th√™m synthetic t·ª´ c√°c c√¢u canonical built-in.")
    ap.add_argument("--template_variants_per", type=int, default=6)

    # train hparams
    ap.add_argument("--batch_size", type=int, default=8)
    ap.add_argument("--lr", type=float, default=2e-5)
    ap.add_argument("--epochs", type=int, default=5)
    ap.add_argument("--fp16", action="store_true")
    ap.add_argument("--bf16", action="store_true")
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--gradient_checkpointing", action="store_true")
    ap.add_argument("--grad_accum", type=int, default=2)

    # infer
    ap.add_argument("--predict", default=None)
    ap.add_argument("--gen_beams", type=int, default=6)
    ap.add_argument("--gen_maxlen", type=int, default=96)
    ap.add_argument("--gen_minlen", type=int, default=0)
    ap.add_argument("--no_repeat_ngram", type=int, default=4)
    ap.add_argument("--length_penalty", type=float, default=1.1)
    ap.add_argument("--repetition_penalty", type=float, default=1.2)
    ap.add_argument("--forbid_slang_at_decode", action="store_true",
                    help="C·∫•m vcl/vkl/vl, emoji v√† <extra_id_*> khi generate (decode constraint, kh√¥ng h·∫≠u x·ª≠ l√Ω).")

    args = ap.parse_args()
    random.seed(args.seed); np.random.seed(args.seed)
    torch.backends.cuda.matmul.allow_tf32 = True
    try: torch.set_float32_matmul_precision("high")
    except Exception: pass

    # ===== tokenizer/model =====
    def load_tok_mdl(name_or_dir: str):
        tok = AutoTokenizer.from_pretrained(name_or_dir, use_fast=False)
        mdl = AutoModelForSeq2SeqLM.from_pretrained(name_or_dir)
        return tok, mdl

    # ===================== INFER =====================
    if args.mode == "infer":
        model_dir = args.output_dir if os.path.isdir(args.output_dir) else args.model_name
        tok, mdl = load_tok_mdl(model_dir)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        mdl.to(device).eval()
        if not args.predict:
            raise SystemExit("C·∫ßn --predict \"chu·ªói c·∫ßn chu·∫©n ho√°\"")

        enc = tok([args.predict], truncation=True, max_length=args.gen_maxlen, return_tensors="pt").to(device)

        # --- C·∫•u h√¨nh ch·∫∑n ---
        bad_words_ids = None
        logits_processors = None

        if args.forbid_slang_at_decode:
            bad_words_ids = []
            # 1) Ch·∫∑n to√†n b·ªô sentinel tokens <extra_id_0..99>
            for i in range(100):
                toks = tok.encode(f"<extra_id_{i}>", add_special_tokens=False)
                if toks:
                    bad_words_ids.append(toks)

            # 2) Ch·∫∑n slang theo chu·ªói token (n·∫øu tokenizer gh√©p ƒë∆∞·ª£c nguy√™n chu·ªói)
            for bad in sorted(SLANG_NEG):
                toks = tok.encode(bad, add_special_tokens=False)
                if toks:
                    bad_words_ids.append(toks)

            # 3) Ch·∫∑n emoji ·ªü m·ª©c token (nhi·ªÅu emoji l√† 1 token)
            for emo in EMO_POS + EMO_NEG:
                toks = tok.encode(emo, add_special_tokens=False)
                if toks:
                    bad_words_ids.append(toks)

            # 4) Ch·∫∑n theo chu·ªói con (m·∫°nh nh·∫•t)
            banned_substrings = set()
            # slang + c√°c bi·∫øn th·ªÉ s√°t nghƒ©a
            for s in ["vcl", "vkl", "vl"]:
                banned_substrings.add(s)
                banned_substrings.add(" " + s)
                banned_substrings.add(s + " ")
                banned_substrings.add(" v kl")  # m·ªôt bi·∫øn th·ªÉ hay g·∫∑p

            # emoji (n·∫øu mu·ªën output s·∫°ch kh√¥ng emoji)
            for emo in EMO_POS + EMO_NEG:
                banned_substrings.add(emo)
                banned_substrings.add(" " + emo)

            logits_processors = LogitsProcessorList([
                ForbidSubstringsProcessor(tok, list(banned_substrings))
            ])

        with torch.no_grad():
            out = mdl.generate(
                **enc,
                max_length=args.gen_maxlen,
                min_length=args.gen_minlen,
                num_beams=args.gen_beams,
                no_repeat_ngram_size=args.no_repeat_ngram,
                length_penalty=args.length_penalty,
                repetition_penalty=args.repetition_penalty,
                early_stopping=True,
                bad_words_ids=bad_words_ids,
                logits_processor=logits_processors,
                # forced_eos_token_id=tok.eos_token_id,  # m·ªü n·∫øu mu·ªën ch·∫Øc ch·∫Øn k·∫øt th√∫c b·∫±ng EOS
            )
        print("[PRED]", tok.decode(out[0], skip_special_tokens=True).strip())
        return

    # ===================== TRAIN =====================
    # 1) d·ªØ li·ªáu t·ª´ CSV/JSONL/TXT s·∫°ch
    if args.train_csv:
        if args.dev_csv:
            ds = load_dataset("csv", data_files={"train": args.train_csv, "validation": args.dev_csv})
        else:
            tmp = load_dataset("csv", data_files={"full": args.train_csv})["full"]
            split = tmp.train_test_split(test_size=args.split_ratio, seed=args.seed)
            ds = DatasetDict(train=split["train"], validation=split["test"])

        def _clean(b):
            return {"src": [nfc(str(x).strip()) for x in b["src"]],
                    "tgt": [nfc(str(x).strip()) for x in b["tgt"]]}
        for split_name in ds.keys():
            cols = set(ds[split_name].column_names)
            if not {"src","tgt"}.issubset(cols):
                raise ValueError(f"[{split_name}] CSV c·∫ßn c·ªôt 'src' & 'tgt', th·∫•y: {cols}")
        ds = ds.map(_clean, batched=True)

    elif args.train_jsonl:
        def _gen(path):
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    j = json.loads(line)
                    yield {"src": nfc(j["src"]).strip(), "tgt": nfc(j["tgt"]).strip()}
        items = list(_gen(args.train_jsonl))
        cut = int(0.9 * len(items)) if len(items) > 1 else 1
        ds = DatasetDict(
            train=Dataset.from_list(items[:cut]),
            validation=Dataset.from_list(items[cut:])
        )

    elif args.clean_txt:
        # clean_txt -> dirtyfy ƒë·ªÉ t·∫°o c·∫∑p (src b·∫©n, tgt s·∫°ch)
        src, tgt = [], []
        with open(args.clean_txt, "r", encoding="utf-8") as f:
            lines = [nfc(x.strip()) for x in f if x.strip()]
        for line in lines:
            for _ in range(args.num_variants):
                src.append(dirtyfy(line)); tgt.append(line)
        base = Dataset.from_dict({"src": src, "tgt": tgt})
        base = base.train_test_split(test_size=0.1, seed=args.seed)
        ds = DatasetDict(train=base["train"], validation=base["test"])

    else:
        default_train = "/home/dat/llm_ws/data/train/mt5_norm_train.csv"
        default_dev   = "/home/dat/llm_ws/data/train/mt5_norm_dev.csv"
        if os.path.exists(default_train) and os.path.exists(default_dev):
            ds = load_dataset("csv", data_files={"train": default_train, "validation": default_dev})
        else:
            raise ValueError("C·∫ßn --train_csv [--dev_csv] ho·∫∑c --train_jsonl ho·∫∑c --clean_txt.")

    # 2) tr·ªôn th√™m synthetic t·ª´ template canonical (r·∫•t quan tr·ªçng)
    if args.use_builtin_templates:
        synth = build_synth_from_templates(BUILTIN_TEMPLATES, variants_per=args.template_variants_per)
        # concat
        train = Dataset.from_dict({
            "src": list(ds["train"]["src"]) + list(synth["train"]["src"]),
            "tgt": list(ds["train"]["tgt"]) + list(synth["train"]["tgt"]),
        })
        valid = Dataset.from_dict({
            "src": list(ds["validation"]["src"]) + list(synth["validation"]["src"]),
            "tgt": list(ds["validation"]["tgt"]) + list(synth["validation"]["tgt"]),
        })
        ds = DatasetDict(train=train, validation=valid)

    # 3) tokenizer/model
    tok = AutoTokenizer.from_pretrained(args.model_name, use_fast=False)
    tok.padding_side = "right"
    mdl = AutoModelForSeq2SeqLM.from_pretrained(args.model_name)
    if args.gradient_checkpointing:
        mdl.gradient_checkpointing_enable()
        mdl.config.use_cache = False

    data_args = DataArgs(model_name=args.model_name)

    def preprocess(batch):
        model_inputs = tok(batch["src"], max_length=data_args.max_src_len,
                           truncation=True, padding="max_length")
        labels = tok(text_target=batch["tgt"], max_length=data_args.max_tgt_len,
                     truncation=True, padding="max_length")["input_ids"]
        pad_id = tok.pad_token_id
        labels = [[(t if t != pad_id else -100) for t in seq] for seq in labels]
        model_inputs["labels"] = labels
        return model_inputs

    ds_tok = ds.map(preprocess, batched=True, remove_columns=ds["train"].column_names)

    # sanity
    if all(t == -100 for t in ds_tok["train"][0]["labels"]):
        raise RuntimeError("Label to√†n -100 ‚Üí ti·ªÅn x·ª≠ l√Ω sai.")

    collator = DataCollatorForSeq2Seq(tok, model=mdl, label_pad_token_id=-100)

    # preflight loss
    device = "cuda" if torch.cuda.is_available() else "cpu"
    mdl.to(device).train()
    try:
        batch = {k: (torch.tensor(v[:2]).to(device) if isinstance(v, list) else v[:2].to(device))
                 for k, v in {k: ds_tok["train"][k] for k in ["input_ids","attention_mask","labels"]}.items()}
        with torch.no_grad():
            out32 = mdl(**batch)
            base_loss = float(out32.loss.detach())
        print("[CHECK] float32 loss:", base_loss)
    except Exception as e:
        print("[WARN] B·ªè qua preflight loss:", e)

    # train args
    try:
        args_train = Seq2SeqTrainingArguments(
            output_dir=args.output_dir,
            per_device_train_batch_size=args.batch_size,
            per_device_eval_batch_size=args.batch_size,
            learning_rate=args.lr,
            num_train_epochs=args.epochs,
            lr_scheduler_type="linear",
            warmup_ratio=0.05,
            gradient_accumulation_steps=args.grad_accum,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            predict_with_generate=True,
            generation_max_length=data_args.max_tgt_len,
            logging_steps=50,
            save_total_limit=3,
            fp16=args.fp16,
            bf16=args.bf16,
            max_grad_norm=1.0,
            label_smoothing_factor=0.0,
            load_best_model_at_end=True,
            metric_for_best_model="eval_chrF",
            greater_is_better=True,
            seed=args.seed,
            report_to=[],
            gradient_checkpointing=args.gradient_checkpointing,
            optim="adafactor",
        )
    except TypeError:
        print("[WARN] transformers c≈© ‚Üí c·∫•u h√¨nh r√∫t g·ªçn.")
        args_train = Seq2SeqTrainingArguments(
            output_dir=args.output_dir,
            per_device_train_batch_size=args.batch_size,
            per_device_eval_batch_size=args.batch_size,
            learning_rate=args.lr,
            num_train_epochs=args.epochs,
            logging_steps=50,
            save_total_limit=3,
            fp16=args.fp16,
            bf16=args.bf16,
            max_grad_norm=1.0,
            label_smoothing_factor=0.0,
            seed=args.seed,
            report_to=[],
            optim="adafactor",
        )

    def compute_metrics(eval_pred):
        preds, labels = eval_pred
        preds = np.where(preds != -100, preds, tok.pad_token_id)
        labels = np.where(labels != -100, labels, tok.pad_token_id)
        pred_str = tok.batch_decode(preds, skip_special_tokens=True)
        label_str = tok.batch_decode(labels, skip_special_tokens=True)
        scores = [chrF1(r, p) for r, p in zip(label_str, pred_str)]
        return {"chrF": float(np.mean(scores))}

    trainer = Seq2SeqTrainer(
        model=mdl,
        args=args_train,
        train_dataset=ds_tok["train"],
        eval_dataset=ds_tok["validation"],
        data_collator=collator,
        tokenizer=tok,
        compute_metrics=compute_metrics,
    )

    trainer.train()
    trainer.save_model(args.output_dir)
    tok.save_pretrained(args.output_dir)

if __name__ == "__main__":
    main()
